{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import torch.optim as optim\n",
    "import sklearn as sk\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# to convert our data to the BERT language representation & use its vocabulary\n",
    "class OurDataset(Dataset):\n",
    "  def __init__(self, data, len_max):\n",
    "    # expected data input is a pandas dataframe\n",
    "    self.data = data\n",
    "    self.data.reset_index(drop=True, inplace=True)\n",
    "    # self.reviews = self.data['summary']\n",
    "    # self.ratings = self.data['overall']\n",
    "    self.len_max = len_max # the maximum length of a review to consider\n",
    "    self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') # add do_lower_case, if you want to do all lowercase text\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, ind):\n",
    "    review = self.data.loc[ind, 'reviewText']\n",
    "    rating = int(self.data.loc[ind, 'overall']) - 1 # Ratings=1,2,3,4,5 to 0,1,2,3,4\n",
    "    # use the BERT Tokenizer to ensure review is represented similarly\n",
    "    tokens = self.tokenizer.tokenize(review)\n",
    "    # recall that BERT uses additional token embeddings\n",
    "    tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
    "    # [CLS] should be added to the beginning of the input\n",
    "    # [SEP] should be added to the end of the input\n",
    "    # to add [PAD] if sentence is too short\n",
    "    if len(tokens) < self.len_max:\n",
    "      # At the end of the tokens add PAD\n",
    "      tokens = tokens + ['[PAD]' for i in range(self.len_max - len(tokens))]\n",
    "    else:\n",
    "      # tokens list is too long, need to cut off the tokens and then re-add SEP\n",
    "      tokens = tokens[:self.len_max - 1] + ['[SEP]']\n",
    "\n",
    "    # Converts tokens to an id using the vocabulary\n",
    "    token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "    # convert to PyTorch tensor\n",
    "    tokens_to_tensors = torch.tensor(token_ids)\n",
    "    # MLM to distinguish between the PAD and the important tokens\n",
    "    attention_mask = (tokens_to_tensors != 0).long()\n",
    "    # convert our labels into tensors\n",
    "    # label = torch.tensor(rating).long()\n",
    "    return tokens_to_tensors, attention_mask, rating\n",
    "\n",
    "\n",
    "# to import the pretrained BERT Model\n",
    "# purpose: multi-class classification where we try to predict the ratings\n",
    "class RatingPredictor(nn.Module):\n",
    "  def __init__(self, rating_scale):\n",
    "    super(RatingPredictor, self).__init__()\n",
    "    # load the BERT Model configuration, and update to match our dataset\n",
    "    # self.bert_config = BertConfig(hidden_size=768,\n",
    "    #                               num_hidden_layers=12,\n",
    "    #                               num_attention_heads=12,\n",
    "    #                               intermediate_size=3072,\n",
    "    #                               num_labels=rating_scale) # change to multi-class\n",
    "    # load in pretrained model\n",
    "    self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "    # self.bert = BertForSequenceClassification('bert-base-uncased', do_lower_case=True, num_labels=rating_scale) # this one has a linear layer after the pooled layer\n",
    "    # because we want to fine-tune, make sure that the weights from BERT aren't updated\n",
    "    for param in self.bert.parameters():\n",
    "      param.requires_grad = False\n",
    "    \n",
    "    # our classifer on top of the BERT Model\n",
    "    self.linear1 = nn.Linear(768, 500)\n",
    "    self.relu = nn.ReLU()\n",
    "    self.drop = nn.Dropout(0.2) # dropout with 50%\n",
    "    self.linear2 = nn.Linear(500, rating_scale)\n",
    "    # self.fc = nn.LogSoftmax(dim=0) # to calculate the probabilities\n",
    "    self.fc = nn.LogSoftmax(dim=1)\n",
    "\n",
    "  def forward(self, tokens, attention_mask):\n",
    "    # grab the BERT Model outputs after forward pass\n",
    "    outputs = self.bert.forward(input_ids=tokens, attention_mask=attention_mask) \n",
    "    # forward pass for Bert will return two outputs\n",
    "    # 12 layers to one pooled output, so grab last output layer\n",
    "    # pooled output size should be (1, 768) as we pass one review at a time\n",
    "    pooled_output = outputs.pooler_output\n",
    "    #print(outputs.pooler_output)\n",
    "    lin_output1 = self.linear1(pooled_output)\n",
    "    relu_output = self.relu(lin_output1)\n",
    "    dropped_output = self.drop(relu_output)\n",
    "    lin_output2 = self.linear2(dropped_output)\n",
    "    result = self.fc(lin_output2)\n",
    "    return result\n",
    "\n",
    "\"\"\"\n",
    "  def forward(self, tokens, attention_mask):\n",
    "    # grab the BERT Model outputs after forward pass\n",
    "    output = self.bert(input_ids=tokens, attention_mask=attention_mask, return_dict=True) \n",
    "    # forward pass for Bert will return two outputs\n",
    "    # 12 layers to one pooled output, so grab last output layer\n",
    "    # pooled output size should be (1, 768) as we pass one review at a time\n",
    "    # print(output.pooler_output[0].size())\n",
    "    pooled_output = output['pooler_output'][0]\n",
    "    lin_output1 = self.linear1(pooled_output)\n",
    "    print(\"through first linear layer\")\n",
    "    relu_output = self.relu(lin_output1)\n",
    "    print(\"ReLu'ed!!\")\n",
    "    dropped_output = self.drop(relu_output)\n",
    "    print(\"dropped bish\")\n",
    "    lin_output2 = self.linear2(dropped_output)\n",
    "    print(\"Linear 2, almost there\")\n",
    "    print(lin_output2.size())\n",
    "    result = self.fc(lin_output2)\n",
    "    print(\"Log soft max completo\")\n",
    "    print(result.size())\n",
    "    return result\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "# training and validation functions\n",
    "def dataloader(fileName, bs):\n",
    "  \"\"\"Load the data into PyTorch DataLoader for train, val, test.\n",
    "  \"\"\"\n",
    "  np.random.seed(42)\n",
    "  # load data from fileName\n",
    "  raw_data = pd.read_json(fileName, lines=True, orient='columns', dtype=True)\n",
    "  raw_data = raw_data[['reviewText', 'overall']]\n",
    "  raw_data = raw_data.dropna()\n",
    "  # split the data into train, val, and test\n",
    "  # reduce the original 200,000 to 50,000, with the original proportions of each class\n",
    "  X_temp1, X_temp2, y_temp1, y_temp2 = train_test_split(raw_data['reviewText'], raw_data['overall'], test_size=0.25, stratify=raw_data['overall'], random_state=42)\n",
    "  # now we have 50,000 examples in  X_temp2, y_temp2\n",
    "  X_train, X_temp3, y_train, y_temp3 = train_test_split(X_temp2, y_temp2, test_size=0.5, stratify=y_temp2, random_state=42)\n",
    "  # X_train now has 25,000 examples\n",
    "  X_val, X_test, y_val, y_test = train_test_split(X_temp3, y_temp3, test_size=0.5, stratify=y_temp3, random_state=42)\n",
    "  # X_val & X_test have 12,500 examples each\n",
    "  # merge X and ys to feed into OurDataset\n",
    "  train_set = pd.DataFrame(X_train)\n",
    "  train_set['overall'] = y_train\n",
    "  val_set = pd.DataFrame(X_val)\n",
    "  val_set['overall'] = y_val\n",
    "  test_set = pd.DataFrame(X_test)\n",
    "  test_set['overall'] = y_test\n",
    "  # train_set = raw_data.sample(frac=0.5, random_state=42)\n",
    "  # temp = raw_data.drop(train_set.index)\n",
    "  # val_set = temp.sample(frac=0.3, random_state=42)\n",
    "  # test_set = temp.drop(val_set.index)\n",
    "  # train_split = 0.5\n",
    "  # val_split = 0.3\n",
    "  # fullsize = len(raw_data)\n",
    "  # indices = list(range(fullsize))\n",
    "  # split1 = int(np.floor(train_split * fullsize))\n",
    "  # split2 = int(np.floor(val_split * fullsize))\n",
    "  # np.random.shuffle(indices)\n",
    "  # train_ind, val_ind, test_ind = indices[:split1], indices[split1:split1+split2], indices[split1+split2:]\n",
    "  # using the split indices, get the samples\n",
    "  # train_sampler = torch.utils.data.SubsetRandomSampler(train_ind)\n",
    "  # val_sampler = torch.utils.data.SubsetRandomSampler(val_ind)\n",
    "  # test_sampler = torch.utils.data.SubsetRandomSampler(test_ind)\n",
    "  # utilize OurDataset class to create & tokenize the data\n",
    "  # all_data = OurDataset(raw_data, 186)\n",
    "  train_data = OurDataset(train_set, 512)\n",
    "  val_data = OurDataset(val_set, 512)\n",
    "  test_data = OurDataset(test_set, 512)\n",
    "  # use DataLoader\n",
    "  train_loader = DataLoader(train_data, batch_size=bs, shuffle=False)\n",
    "  val_loader = DataLoader(val_data, batch_size=bs, shuffle=False)\n",
    "  test_loader = DataLoader(test_data, batch_size=bs) # ??\n",
    "  return train_loader, val_loader, test_loader, len(train_set), len(val_set), len(test_set)\n",
    "\n",
    "\n",
    "def get_accuracy(pred, label):\n",
    "  # determine the index of the most likely rating\n",
    "  index = torch.argmax(pred, dim = 1)\n",
    "  # return the number of correctly predicted ratings / number of total examples in a batch\n",
    "  return (index==label).sum().item()\n",
    "\n",
    "\n",
    "def evaluate(model, loader, loader_len, criterion):\n",
    "  \"\"\"Evaluate the network model based on validation set.\n",
    "  \"\"\"\n",
    "  #model.train(False)\n",
    "  model.eval() # go into evaluation mode\n",
    "  acc, err = 0, 0\n",
    "  with torch.no_grad():\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    for iter, (tokens, attention_mask, rating) in enumerate(loader):\n",
    "      pred = model(tokens, attention_mask)\n",
    "      # loss = criterion(nn.LogSoftmax(pred, dim=1), rating)\n",
    "      loss = criterion(pred, rating)\n",
    "      total_loss += loss.item()\n",
    "      total_acc += get_accuracy(pred, rating)\n",
    "      if (iter + 1) % 100 == 0:\n",
    "        print(\"Iter {}     -     Loss: {}     -      Accuracy: {}\".format(iter+1, total_loss / (iter+1), total_acc / (iter*32)))\n",
    "\n",
    "    err = (total_loss) / (iter + 1)\n",
    "    acc = (total_acc) / (loader_len) # the total number of correctly predicted \n",
    "  return err, acc\n",
    "\n",
    "\n",
    "def train(model, train_loader, train_len, val_loader, val_len, epochs, learning_rate):\n",
    "  \"\"\"Use training and validation, train the model.\n",
    "  \"\"\"\n",
    "  print(\"I'm at the start\")\n",
    "  model.train(True)\n",
    "  torch.manual_seed(42) # for reproducibility\n",
    "  # define loss function and optimizer for weight updates\n",
    "  target_weights = torch.FloatTensor([20/41, 19/41, 8/41, 3/41, 1/41]) # Weights that should be multiplied to the learning rate of the optimizer\n",
    "  criterion = nn.NLLLoss(weight=target_weights)\n",
    "  optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "  # to store values later\n",
    "  train_acc, train_loss, val_acc, val_loss = [], [], [], []\n",
    "  startTime = time.time()\n",
    "  for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    # iterate through the batches\n",
    "    # iter = 0\n",
    "    for iter, (tokens, attention_mask, rating) in enumerate(train_loader):\n",
    "      optimizer.zero_grad()\n",
    "      pred = model(tokens, attention_mask) # predict using tokens & attention mask\n",
    "      # compute loss\n",
    "      # loss = criterion(nn.LogSoftmax(pred, dim=1), rating)\n",
    "      loss = criterion(pred, rating)\n",
    "      # backprop\n",
    "      loss.backward()\n",
    "      # weight updates\n",
    "      optimizer.step()\n",
    "      # add in loss and accuracy (number of correctly predicted ratings)\n",
    "      total_loss += (loss)\n",
    "      total_acc += (get_accuracy(pred, rating))\n",
    "    \n",
    "      # for us to see where we are in training\n",
    "      if (iter+1) % 10 == 0:\n",
    "        print(\"Epoch {}  - Iter {}  - Training Time: {} -     Loss: {}      -    Accuracy: {}\".format(epoch+1, iter+1, time.time()-startTime,\n",
    "        total_loss / (iter+1), total_acc / (iter*32)))\n",
    "    train_loss.append(total_loss / (iter+1)) # calculate the average loss across all iterations per epoch\n",
    "    train_acc.append(total_acc / train_len) # calculate the number of correctly predicted ratings / total training examples\n",
    "    total_loss = 0.0\n",
    "    # compute validation loss at the end of each epoch\n",
    "    val_err, val_avg_acc = evaluate(model, val_loader, val_len, criterion)\n",
    "    val_loss.append(val_err)\n",
    "    val_acc.append(val_avg_acc)\n",
    "\n",
    "    print(\"END  ---  Epoch {}  ---  Training Error: {}  ---   Validation Error: {}\".format(\n",
    "        epoch, train_loss[epoch], val_err))\n",
    "  return train_loss, train_acc, val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish loading our data splits!\n"
     ]
    }
   ],
   "source": [
    "# load the data for training and validation, set aside the test\n",
    "train_loader, val_loader, test_loader, train_len, val_len, test_len = dataloader('train.json', bs=32)\n",
    "print(\"Finish loading our data splits!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiated Instance of Our Network\n"
     ]
    }
   ],
   "source": [
    "mod = RatingPredictor(rating_scale=5)\n",
    "print(\"Initiated Instance of Our Network\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm at the start\n",
      "Epoch 1  - Iter 10  - Training Time: 407.6044840812683 -     Loss: 1.6166388988494873      -    Accuracy: 0.28125\n",
      "Epoch 1  - Iter 20  - Training Time: 817.4286618232727 -     Loss: 1.607580542564392      -    Accuracy: 0.26151315789473684\n",
      "Epoch 1  - Iter 30  - Training Time: 1234.8846859931946 -     Loss: 1.6130540370941162      -    Accuracy: 0.2273706896551724\n",
      "Epoch 1  - Iter 40  - Training Time: 1641.68985581398 -     Loss: 1.6126466989517212      -    Accuracy: 0.22275641025641027\n",
      "Epoch 1  - Iter 50  - Training Time: 2053.19322681427 -     Loss: 1.6148546934127808      -    Accuracy: 0.22385204081632654\n",
      "Epoch 1  - Iter 60  - Training Time: 2450.1118021011353 -     Loss: 1.6114085912704468      -    Accuracy: 0.21927966101694915\n",
      "Epoch 1  - Iter 70  - Training Time: 2871.865550994873 -     Loss: 1.6120229959487915      -    Accuracy: 0.213768115942029\n",
      "Epoch 1  - Iter 80  - Training Time: 3272.555946826935 -     Loss: 1.6111360788345337      -    Accuracy: 0.22626582278481014\n",
      "Epoch 1  - Iter 90  - Training Time: 3666.050936937332 -     Loss: 1.6101723909378052      -    Accuracy: 0.24578651685393257\n",
      "Epoch 1  - Iter 100  - Training Time: 4045.7633459568024 -     Loss: 1.606233835220337      -    Accuracy: 0.2803030303030303\n",
      "Epoch 1  - Iter 110  - Training Time: 4437.383641958237 -     Loss: 1.603458046913147      -    Accuracy: 0.3038990825688073\n",
      "Epoch 1  - Iter 120  - Training Time: 4825.677602052689 -     Loss: 1.603486180305481      -    Accuracy: 0.32090336134453784\n",
      "Epoch 1  - Iter 130  - Training Time: 5214.896542072296 -     Loss: 1.60472571849823      -    Accuracy: 0.3282461240310077\n",
      "Epoch 1  - Iter 140  - Training Time: 5604.858174085617 -     Loss: 1.603825569152832      -    Accuracy: 0.3262140287769784\n",
      "Epoch 1  - Iter 150  - Training Time: 5994.050241947174 -     Loss: 1.6039701700210571      -    Accuracy: 0.32172818791946306\n",
      "Epoch 1  - Iter 160  - Training Time: 6383.616522073746 -     Loss: 1.602370023727417      -    Accuracy: 0.32075471698113206\n",
      "Epoch 1  - Iter 170  - Training Time: 6772.809946060181 -     Loss: 1.6013023853302002      -    Accuracy: 0.32747781065088755\n",
      "Epoch 1  - Iter 180  - Training Time: 7161.788625955582 -     Loss: 1.600154161453247      -    Accuracy: 0.3432262569832402\n",
      "Epoch 1  - Iter 190  - Training Time: 7550.830436944962 -     Loss: 1.5981591939926147      -    Accuracy: 0.3578042328042328\n",
      "Epoch 1  - Iter 200  - Training Time: 7939.975673913956 -     Loss: 1.5960593223571777      -    Accuracy: 0.3696608040201005\n",
      "Epoch 1  - Iter 210  - Training Time: 8329.4234521389 -     Loss: 1.5966025590896606      -    Accuracy: 0.3785885167464115\n",
      "Epoch 1  - Iter 220  - Training Time: 8718.58637714386 -     Loss: 1.5944195985794067      -    Accuracy: 0.3827054794520548\n",
      "Epoch 1  - Iter 230  - Training Time: 9107.678692102432 -     Loss: 1.5942498445510864      -    Accuracy: 0.38195960698689957\n",
      "Epoch 1  - Iter 240  - Training Time: 9497.552747964859 -     Loss: 1.5942437648773193      -    Accuracy: 0.3820606694560669\n",
      "Epoch 1  - Iter 250  - Training Time: 9887.073334932327 -     Loss: 1.5928735733032227      -    Accuracy: 0.3839106425702811\n",
      "Epoch 1  - Iter 260  - Training Time: 10276.651649951935 -     Loss: 1.593401551246643      -    Accuracy: 0.3858590733590734\n",
      "Epoch 1  - Iter 270  - Training Time: 10667.250277042389 -     Loss: 1.5928456783294678      -    Accuracy: 0.3888243494423792\n",
      "Epoch 1  - Iter 280  - Training Time: 11067.482473134995 -     Loss: 1.5923056602478027      -    Accuracy: 0.39034498207885304\n",
      "Epoch 1  - Iter 290  - Training Time: 11458.429589033127 -     Loss: 1.5920597314834595      -    Accuracy: 0.3874351211072664\n",
      "Epoch 1  - Iter 300  - Training Time: 11849.629589796066 -     Loss: 1.5921075344085693      -    Accuracy: 0.38482441471571904\n",
      "Epoch 1  - Iter 310  - Training Time: 12244.435701847076 -     Loss: 1.592596173286438      -    Accuracy: 0.38197815533980584\n",
      "Epoch 1  - Iter 320  - Training Time: 12632.244451999664 -     Loss: 1.5914238691329956      -    Accuracy: 0.38254310344827586\n",
      "Epoch 1  - Iter 330  - Training Time: 13015.945886135101 -     Loss: 1.591443419456482      -    Accuracy: 0.38288373860182373\n",
      "Epoch 1  - Iter 340  - Training Time: 13400.272939920425 -     Loss: 1.5911985635757446      -    Accuracy: 0.38163716814159293\n",
      "Epoch 1  - Iter 350  - Training Time: 13785.822767972946 -     Loss: 1.5900381803512573      -    Accuracy: 0.38126790830945556\n",
      "Epoch 1  - Iter 360  - Training Time: 14171.934933185577 -     Loss: 1.5891730785369873      -    Accuracy: 0.3826601671309192\n",
      "Epoch 1  - Iter 370  - Training Time: 14558.373550891876 -     Loss: 1.5883187055587769      -    Accuracy: 0.3866869918699187\n",
      "Epoch 1  - Iter 380  - Training Time: 14947.504004955292 -     Loss: 1.5882338285446167      -    Accuracy: 0.3912434036939314\n",
      "Epoch 1  - Iter 390  - Training Time: 15364.406563043594 -     Loss: 1.588365912437439      -    Accuracy: 0.3949228791773779\n",
      "Epoch 1  - Iter 400  - Training Time: 15809.665307044983 -     Loss: 1.5879101753234863      -    Accuracy: 0.39614661654135336\n",
      "Epoch 1  - Iter 410  - Training Time: 16248.965673208237 -     Loss: 1.587331771850586      -    Accuracy: 0.3976161369193154\n",
      "Epoch 1  - Iter 420  - Training Time: 16680.012092113495 -     Loss: 1.586419701576233      -    Accuracy: 0.4011038186157518\n",
      "Epoch 1  - Iter 430  - Training Time: 17108.989780187607 -     Loss: 1.5864534378051758      -    Accuracy: 0.4039189976689977\n",
      "Epoch 1  - Iter 440  - Training Time: 17543.778574943542 -     Loss: 1.586654782295227      -    Accuracy: 0.40639236902050113\n",
      "Epoch 1  - Iter 450  - Training Time: 17969.80689406395 -     Loss: 1.586504578590393      -    Accuracy: 0.40625\n",
      "Epoch 1  - Iter 460  - Training Time: 18385.71396589279 -     Loss: 1.5854390859603882      -    Accuracy: 0.4076797385620915\n",
      "Epoch 1  - Iter 470  - Training Time: 18820.313667058945 -     Loss: 1.5850286483764648      -    Accuracy: 0.4107809168443497\n",
      "Epoch 1  - Iter 480  - Training Time: 19238.206373929977 -     Loss: 1.585702896118164      -    Accuracy: 0.41062108559498955\n",
      "Epoch 1  - Iter 490  - Training Time: 19651.736554145813 -     Loss: 1.585782766342163      -    Accuracy: 0.4101482617586912\n",
      "Epoch 1  - Iter 500  - Training Time: 20061.983210086823 -     Loss: 1.5850619077682495      -    Accuracy: 0.40837925851703405\n",
      "Epoch 1  - Iter 510  - Training Time: 20469.86428618431 -     Loss: 1.585344910621643      -    Accuracy: 0.40729371316306484\n",
      "Epoch 1  - Iter 520  - Training Time: 20888.05828690529 -     Loss: 1.5851020812988281      -    Accuracy: 0.40715317919075145\n",
      "Epoch 1  - Iter 530  - Training Time: 21303.082531929016 -     Loss: 1.5848764181137085      -    Accuracy: 0.4071361058601134\n",
      "Epoch 1  - Iter 540  - Training Time: 21724.944928884506 -     Loss: 1.5843628644943237      -    Accuracy: 0.407293599257885\n",
      "Epoch 1  - Iter 550  - Training Time: 22144.848860025406 -     Loss: 1.58359694480896      -    Accuracy: 0.4089822404371585\n",
      "Epoch 1  - Iter 560  - Training Time: 22555.56231403351 -     Loss: 1.5831718444824219      -    Accuracy: 0.4087097495527728\n",
      "Epoch 1  - Iter 570  - Training Time: 22970.45439505577 -     Loss: 1.5828285217285156      -    Accuracy: 0.40795254833040423\n",
      "Epoch 1  - Iter 580  - Training Time: 23388.539937973022 -     Loss: 1.5818556547164917      -    Accuracy: 0.40851683937823835\n",
      "Epoch 1  - Iter 590  - Training Time: 23797.47685098648 -     Loss: 1.5816417932510376      -    Accuracy: 0.4112372665534805\n",
      "Epoch 1  - Iter 600  - Training Time: 24208.98748087883 -     Loss: 1.581519603729248      -    Accuracy: 0.4136060100166945\n",
      "Epoch 1  - Iter 610  - Training Time: 24609.517755031586 -     Loss: 1.5811362266540527      -    Accuracy: 0.41564039408866993\n",
      "Epoch 1  - Iter 620  - Training Time: 25011.4421210289 -     Loss: 1.5807610750198364      -    Accuracy: 0.41584208400646205\n",
      "Epoch 1  - Iter 630  - Training Time: 25435.26411294937 -     Loss: 1.5802167654037476      -    Accuracy: 0.4162360890302067\n",
      "Epoch 1  - Iter 640  - Training Time: 25878.71001791954 -     Loss: 1.5796544551849365      -    Accuracy: 0.417693661971831\n",
      "Epoch 1  - Iter 650  - Training Time: 26305.098910093307 -     Loss: 1.5795586109161377      -    Accuracy: 0.41876926040061635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  - Iter 660  - Training Time: 26714.764316082 -     Loss: 1.5793476104736328      -    Accuracy: 0.41952769347496205\n",
      "Epoch 1  - Iter 670  - Training Time: 27132.267019987106 -     Loss: 1.5790681838989258      -    Accuracy: 0.4198430493273543\n",
      "Epoch 1  - Iter 680  - Training Time: 27541.97731399536 -     Loss: 1.5784904956817627      -    Accuracy: 0.41927466863033874\n",
      "Epoch 1  - Iter 690  - Training Time: 27959.15756201744 -     Loss: 1.5781468152999878      -    Accuracy: 0.4187681422351234\n",
      "Epoch 1  - Iter 700  - Training Time: 28375.442598104477 -     Loss: 1.5778794288635254      -    Accuracy: 0.4182761087267525\n",
      "Epoch 1  - Iter 710  - Training Time: 28785.748197078705 -     Loss: 1.5774352550506592      -    Accuracy: 0.41797425952045136\n",
      "Epoch 1  - Iter 720  - Training Time: 29192.068790912628 -     Loss: 1.5775600671768188      -    Accuracy: 0.4184196801112656\n",
      "Epoch 1  - Iter 730  - Training Time: 29614.55724787712 -     Loss: 1.577069640159607      -    Accuracy: 0.41919581618655694\n",
      "Epoch 1  - Iter 740  - Training Time: 30022.095362901688 -     Loss: 1.5769108533859253      -    Accuracy: 0.4199086603518268\n",
      "Epoch 1  - Iter 750  - Training Time: 30449.00016593933 -     Loss: 1.576605200767517      -    Accuracy: 0.4191421895861148\n",
      "Epoch 1  - Iter 760  - Training Time: 30836.295353889465 -     Loss: 1.5761245489120483      -    Accuracy: 0.41716073781291174\n",
      "Epoch 1  - Iter 770  - Training Time: 31224.644504070282 -     Loss: 1.5761723518371582      -    Accuracy: 0.416896944083225\n",
      "Epoch 1  - Iter 780  - Training Time: 31633.515360832214 -     Loss: 1.5760148763656616      -    Accuracy: 0.4186055840821566\n",
      "Iter 100     -     Loss: 1.5349300181865693     -      Accuracy: 0.5909090909090909\n",
      "Iter 200     -     Loss: 1.5373056900501252     -      Accuracy: 0.5890389447236181\n",
      "Iter 300     -     Loss: 1.534976006746292     -      Accuracy: 0.590510033444816\n",
      "END  ---  Epoch 0  ---  Training Error: 1.5754996538162231  ---   Validation Error: 1.5332056866277515\n",
      "Epoch 2  - Iter 10  - Training Time: 45730.40341591835 -     Loss: 1.5862517356872559      -    Accuracy: 0.6736111111111112\n",
      "Epoch 2  - Iter 20  - Training Time: 46060.71976304054 -     Loss: 1.5567001104354858      -    Accuracy: 0.5904605263157895\n",
      "Epoch 2  - Iter 30  - Training Time: 46390.986605882645 -     Loss: 1.5515979528427124      -    Accuracy: 0.47844827586206895\n",
      "Epoch 2  - Iter 40  - Training Time: 46722.226355075836 -     Loss: 1.5495752096176147      -    Accuracy: 0.4423076923076923\n",
      "Epoch 2  - Iter 50  - Training Time: 47052.778608083725 -     Loss: 1.5500991344451904      -    Accuracy: 0.4413265306122449\n",
      "Epoch 2  - Iter 60  - Training Time: 47383.795457839966 -     Loss: 1.5469309091567993      -    Accuracy: 0.4322033898305085\n",
      "Epoch 2  - Iter 70  - Training Time: 47714.88445401192 -     Loss: 1.5504021644592285      -    Accuracy: 0.4230072463768116\n",
      "Epoch 2  - Iter 80  - Training Time: 48045.562649965286 -     Loss: 1.5482041835784912      -    Accuracy: 0.43156645569620256\n",
      "Epoch 2  - Iter 90  - Training Time: 48377.0254650116 -     Loss: 1.5459545850753784      -    Accuracy: 0.4417134831460674\n",
      "Epoch 2  - Iter 100  - Training Time: 48708.28091406822 -     Loss: 1.5416667461395264      -    Accuracy: 0.45580808080808083\n",
      "Epoch 2  - Iter 110  - Training Time: 49039.29726409912 -     Loss: 1.5381724834442139      -    Accuracy: 0.4598623853211009\n",
      "Epoch 2  - Iter 120  - Training Time: 49369.32821106911 -     Loss: 1.5379010438919067      -    Accuracy: 0.46244747899159666\n",
      "Epoch 2  - Iter 130  - Training Time: 49699.45200800896 -     Loss: 1.537261962890625      -    Accuracy: 0.45978682170542634\n",
      "Epoch 2  - Iter 140  - Training Time: 50029.85551905632 -     Loss: 1.5357824563980103      -    Accuracy: 0.4568345323741007\n",
      "Epoch 2  - Iter 150  - Training Time: 50360.520959854126 -     Loss: 1.5348485708236694      -    Accuracy: 0.45721476510067116\n",
      "Epoch 2  - Iter 160  - Training Time: 50691.053029060364 -     Loss: 1.5339112281799316      -    Accuracy: 0.4563679245283019\n",
      "Epoch 2  - Iter 170  - Training Time: 51021.883106946945 -     Loss: 1.5335346460342407      -    Accuracy: 0.4595044378698225\n",
      "Epoch 2  - Iter 180  - Training Time: 51352.690577983856 -     Loss: 1.5327919721603394      -    Accuracy: 0.46875\n",
      "Epoch 2  - Iter 190  - Training Time: 51683.8513109684 -     Loss: 1.5300946235656738      -    Accuracy: 0.47767857142857145\n",
      "Epoch 2  - Iter 200  - Training Time: 52015.34879398346 -     Loss: 1.5298060178756714      -    Accuracy: 0.4808417085427136\n",
      "Epoch 2  - Iter 210  - Training Time: 52347.36627292633 -     Loss: 1.530643105506897      -    Accuracy: 0.48549641148325356\n",
      "Epoch 2  - Iter 220  - Training Time: 52679.12399983406 -     Loss: 1.5286078453063965      -    Accuracy: 0.4850171232876712\n",
      "Epoch 2  - Iter 230  - Training Time: 53010.447468042374 -     Loss: 1.5282262563705444      -    Accuracy: 0.4819868995633188\n",
      "Epoch 2  - Iter 240  - Training Time: 53341.54007601738 -     Loss: 1.5276024341583252      -    Accuracy: 0.48091004184100417\n",
      "Epoch 2  - Iter 250  - Training Time: 53673.433275938034 -     Loss: 1.5264451503753662      -    Accuracy: 0.48167670682730923\n",
      "Epoch 2  - Iter 260  - Training Time: 54004.82621717453 -     Loss: 1.5269750356674194      -    Accuracy: 0.48081563706563707\n",
      "Epoch 2  - Iter 270  - Training Time: 54336.709279060364 -     Loss: 1.526944637298584      -    Accuracy: 0.4819934944237918\n",
      "Epoch 2  - Iter 280  - Training Time: 54668.501479148865 -     Loss: 1.5268679857254028      -    Accuracy: 0.481070788530466\n",
      "Epoch 2  - Iter 290  - Training Time: 55000.466675043106 -     Loss: 1.525879979133606      -    Accuracy: 0.47826557093425603\n",
      "Epoch 2  - Iter 300  - Training Time: 55331.665472984314 -     Loss: 1.5260951519012451      -    Accuracy: 0.4765886287625418\n",
      "Epoch 2  - Iter 310  - Training Time: 55665.15227794647 -     Loss: 1.5273410081863403      -    Accuracy: 0.47582928802589\n",
      "Epoch 2  - Iter 320  - Training Time: 55996.64531493187 -     Loss: 1.5261542797088623      -    Accuracy: 0.4745297805642633\n",
      "Epoch 2  - Iter 330  - Training Time: 56327.00097799301 -     Loss: 1.5261776447296143      -    Accuracy: 0.47416413373860183\n",
      "Epoch 2  - Iter 340  - Training Time: 56656.741183042526 -     Loss: 1.5255584716796875      -    Accuracy: 0.4724373156342183\n",
      "Epoch 2  - Iter 350  - Training Time: 56985.01277089119 -     Loss: 1.5239174365997314      -    Accuracy: 0.471615329512894\n",
      "Epoch 2  - Iter 360  - Training Time: 57311.20554614067 -     Loss: 1.522986650466919      -    Accuracy: 0.4740598885793872\n",
      "Epoch 2  - Iter 370  - Training Time: 57640.418200969696 -     Loss: 1.522454023361206      -    Accuracy: 0.4766260162601626\n",
      "Epoch 2  - Iter 380  - Training Time: 57970.62151002884 -     Loss: 1.522117257118225      -    Accuracy: 0.4795514511873351\n",
      "Epoch 2  - Iter 390  - Training Time: 58301.88402199745 -     Loss: 1.5221257209777832      -    Accuracy: 0.480719794344473\n",
      "Epoch 2  - Iter 400  - Training Time: 58633.16812705994 -     Loss: 1.5216537714004517      -    Accuracy: 0.4786184210526316\n",
      "Epoch 2  - Iter 410  - Training Time: 58964.447519779205 -     Loss: 1.5214184522628784      -    Accuracy: 0.4794468215158924\n",
      "Epoch 2  - Iter 420  - Training Time: 59296.07863402367 -     Loss: 1.5203818082809448      -    Accuracy: 0.4821002386634845\n",
      "Epoch 2  - Iter 430  - Training Time: 59627.69288897514 -     Loss: 1.5199029445648193      -    Accuracy: 0.48382867132867136\n",
      "Epoch 2  - Iter 440  - Training Time: 59959.28364396095 -     Loss: 1.5195447206497192      -    Accuracy: 0.484624145785877\n",
      "Epoch 2  - Iter 450  - Training Time: 60290.59804201126 -     Loss: 1.5196034908294678      -    Accuracy: 0.48308741648106907\n",
      "Epoch 2  - Iter 460  - Training Time: 60621.48959302902 -     Loss: 1.5183708667755127      -    Accuracy: 0.48481753812636164\n",
      "Epoch 2  - Iter 470  - Training Time: 60952.50208187103 -     Loss: 1.518109917640686      -    Accuracy: 0.48740671641791045\n",
      "Epoch 2  - Iter 480  - Training Time: 61283.87832689285 -     Loss: 1.5187652111053467      -    Accuracy: 0.48721294363256784\n",
      "Epoch 2  - Iter 490  - Training Time: 61615.22103905678 -     Loss: 1.5189086198806763      -    Accuracy: 0.48517382413087934\n",
      "Epoch 2  - Iter 500  - Training Time: 61946.73191690445 -     Loss: 1.5184204578399658      -    Accuracy: 0.4839053106212425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  - Iter 510  - Training Time: 62278.38876724243 -     Loss: 1.5187349319458008      -    Accuracy: 0.48428290766208254\n",
      "Epoch 2  - Iter 520  - Training Time: 62609.72949695587 -     Loss: 1.518999695777893      -    Accuracy: 0.4836825626204239\n",
      "Epoch 2  - Iter 530  - Training Time: 62941.54171991348 -     Loss: 1.5190911293029785      -    Accuracy: 0.4820415879017013\n",
      "Epoch 2  - Iter 540  - Training Time: 63272.95648813248 -     Loss: 1.518230676651001      -    Accuracy: 0.48208487940630795\n",
      "Epoch 2  - Iter 550  - Training Time: 63604.127594947815 -     Loss: 1.5168609619140625      -    Accuracy: 0.48372040072859745\n",
      "Epoch 2  - Iter 560  - Training Time: 63935.13967990875 -     Loss: 1.516809105873108      -    Accuracy: 0.48300536672629696\n",
      "Epoch 2  - Iter 570  - Training Time: 64266.55780386925 -     Loss: 1.5164817571640015      -    Accuracy: 0.48259007029876977\n",
      "Epoch 2  - Iter 580  - Training Time: 64597.98266005516 -     Loss: 1.5154531002044678      -    Accuracy: 0.4834304835924007\n",
      "Epoch 2  - Iter 590  - Training Time: 64929.621576070786 -     Loss: 1.5151602029800415      -    Accuracy: 0.48620543293718166\n",
      "Epoch 2  - Iter 600  - Training Time: 65260.928133010864 -     Loss: 1.515290379524231      -    Accuracy: 0.4872704507512521\n",
      "Epoch 2  - Iter 610  - Training Time: 65592.56232213974 -     Loss: 1.5147641897201538      -    Accuracy: 0.48794129720853857\n",
      "Epoch 2  - Iter 620  - Training Time: 65924.6100640297 -     Loss: 1.5140883922576904      -    Accuracy: 0.48768174474959614\n",
      "Epoch 2  - Iter 630  - Training Time: 66255.82029891014 -     Loss: 1.513545036315918      -    Accuracy: 0.488920906200318\n",
      "Epoch 2  - Iter 640  - Training Time: 66586.91621112823 -     Loss: 1.512884497642517      -    Accuracy: 0.4901212832550861\n",
      "Epoch 2  - Iter 650  - Training Time: 66918.71779799461 -     Loss: 1.5129542350769043      -    Accuracy: 0.49022534668721107\n",
      "Epoch 2  - Iter 660  - Training Time: 67250.15136289597 -     Loss: 1.5126991271972656      -    Accuracy: 0.4900891502276176\n",
      "Epoch 2  - Iter 670  - Training Time: 67581.56150484085 -     Loss: 1.512093186378479      -    Accuracy: 0.4898636023916293\n",
      "Epoch 2  - Iter 680  - Training Time: 67913.29239320755 -     Loss: 1.5115927457809448      -    Accuracy: 0.4893685567010309\n",
      "Epoch 2  - Iter 690  - Training Time: 68245.00629401207 -     Loss: 1.5112457275390625      -    Accuracy: 0.4899310595065312\n",
      "Epoch 2  - Iter 700  - Training Time: 68577.54495406151 -     Loss: 1.511250376701355      -    Accuracy: 0.4895386266094421\n",
      "Epoch 2  - Iter 710  - Training Time: 68909.70314311981 -     Loss: 1.5102274417877197      -    Accuracy: 0.4890691114245416\n",
      "Epoch 2  - Iter 720  - Training Time: 69241.64898014069 -     Loss: 1.5103323459625244      -    Accuracy: 0.48930806675938804\n",
      "Epoch 2  - Iter 730  - Training Time: 69573.1433069706 -     Loss: 1.5100146532058716      -    Accuracy: 0.48988340192043894\n",
      "Epoch 2  - Iter 740  - Training Time: 69905.11592411995 -     Loss: 1.5099250078201294      -    Accuracy: 0.48963971583220567\n",
      "Epoch 2  - Iter 750  - Training Time: 70243.58257508278 -     Loss: 1.509809136390686      -    Accuracy: 0.48860981308411217\n",
      "Epoch 2  - Iter 760  - Training Time: 70587.8790910244 -     Loss: 1.5092309713363647      -    Accuracy: 0.4870306324110672\n",
      "Epoch 2  - Iter 770  - Training Time: 70929.29503297806 -     Loss: 1.5094826221466064      -    Accuracy: 0.48793075422626786\n",
      "Epoch 2  - Iter 780  - Training Time: 71280.15984201431 -     Loss: 1.5093770027160645      -    Accuracy: 0.4889682284980745\n",
      "Iter 100     -     Loss: 1.4807418298721313     -      Accuracy: 0.5628156565656566\n",
      "Iter 200     -     Loss: 1.4818967658281326     -      Accuracy: 0.5603015075376885\n",
      "Iter 300     -     Loss: 1.477586297194163     -      Accuracy: 0.5609322742474916\n",
      "END  ---  Epoch 1  ---  Training Error: 1.5087311267852783  ---   Validation Error: 1.4746790392624447\n",
      "Epoch 3  - Iter 10  - Training Time: 85213.56147503853 -     Loss: 1.5337945222854614      -    Accuracy: 0.6458333333333334\n",
      "Epoch 3  - Iter 20  - Training Time: 85569.33095788956 -     Loss: 1.4949114322662354      -    Accuracy: 0.5756578947368421\n",
      "Epoch 3  - Iter 30  - Training Time: 85933.30117797852 -     Loss: 1.4884716272354126      -    Accuracy: 0.49676724137931033\n",
      "Epoch 3  - Iter 40  - Training Time: 86302.46922302246 -     Loss: 1.4877848625183105      -    Accuracy: 0.49439102564102566\n",
      "Epoch 3  - Iter 50  - Training Time: 86679.80225491524 -     Loss: 1.4871388673782349      -    Accuracy: 0.49744897959183676\n",
      "Epoch 3  - Iter 60  - Training Time: 87071.7021920681 -     Loss: 1.4854077100753784      -    Accuracy: 0.4904661016949153\n",
      "Epoch 3  - Iter 70  - Training Time: 87454.96834611893 -     Loss: 1.4908802509307861      -    Accuracy: 0.48641304347826086\n",
      "Epoch 3  - Iter 80  - Training Time: 87839.43339395523 -     Loss: 1.487382411956787      -    Accuracy: 0.49564873417721517\n",
      "Epoch 3  - Iter 90  - Training Time: 88224.2202899456 -     Loss: 1.4829133749008179      -    Accuracy: 0.5031601123595506\n",
      "Epoch 3  - Iter 100  - Training Time: 88604.54033708572 -     Loss: 1.4782711267471313      -    Accuracy: 0.5129419191919192\n",
      "Epoch 3  - Iter 110  - Training Time: 88978.70663785934 -     Loss: 1.473922610282898      -    Accuracy: 0.5163417431192661\n",
      "Epoch 3  - Iter 120  - Training Time: 89353.76526093483 -     Loss: 1.474381923675537      -    Accuracy: 0.5136554621848739\n",
      "Epoch 3  - Iter 130  - Training Time: 89729.5806620121 -     Loss: 1.4734209775924683      -    Accuracy: 0.5079941860465116\n",
      "Epoch 3  - Iter 140  - Training Time: 90094.79699110985 -     Loss: 1.4724465608596802      -    Accuracy: 0.5053956834532374\n",
      "Epoch 3  - Iter 150  - Training Time: 90459.17943000793 -     Loss: 1.4709683656692505      -    Accuracy: 0.5052432885906041\n",
      "Epoch 3  - Iter 160  - Training Time: 90825.18563485146 -     Loss: 1.4705220460891724      -    Accuracy: 0.5045204402515723\n",
      "Epoch 3  - Iter 170  - Training Time: 91194.02763986588 -     Loss: 1.470887541770935      -    Accuracy: 0.5072115384615384\n",
      "Epoch 3  - Iter 180  - Training Time: 91567.75713014603 -     Loss: 1.4699790477752686      -    Accuracy: 0.5160614525139665\n",
      "Epoch 3  - Iter 190  - Training Time: 91931.48178815842 -     Loss: 1.4665493965148926      -    Accuracy: 0.5236441798941799\n",
      "Epoch 3  - Iter 200  - Training Time: 92300.74404811859 -     Loss: 1.467092752456665      -    Accuracy: 0.5233982412060302\n",
      "Epoch 3  - Iter 210  - Training Time: 92668.46203398705 -     Loss: 1.4681720733642578      -    Accuracy: 0.527511961722488\n",
      "Epoch 3  - Iter 220  - Training Time: 93033.61321091652 -     Loss: 1.4655039310455322      -    Accuracy: 0.5285388127853882\n",
      "Epoch 3  - Iter 230  - Training Time: 93396.03936195374 -     Loss: 1.465216040611267      -    Accuracy: 0.5259279475982532\n",
      "Epoch 3  - Iter 240  - Training Time: 93765.2996199131 -     Loss: 1.4639613628387451      -    Accuracy: 0.5240585774058577\n",
      "Epoch 3  - Iter 250  - Training Time: 94129.96075105667 -     Loss: 1.4631117582321167      -    Accuracy: 0.5252259036144579\n",
      "Epoch 3  - Iter 260  - Training Time: 94494.02601790428 -     Loss: 1.4642188549041748      -    Accuracy: 0.5243725868725869\n",
      "Epoch 3  - Iter 270  - Training Time: 94858.4308462143 -     Loss: 1.464598536491394      -    Accuracy: 0.5239312267657993\n",
      "Epoch 3  - Iter 280  - Training Time: 95228.85482311249 -     Loss: 1.4645991325378418      -    Accuracy: 0.5224014336917563\n",
      "Epoch 3  - Iter 290  - Training Time: 95613.34927916527 -     Loss: 1.4635000228881836      -    Accuracy: 0.5194636678200693\n",
      "Epoch 3  - Iter 300  - Training Time: 95998.57791304588 -     Loss: 1.4638758897781372      -    Accuracy: 0.5187081939799331\n",
      "Epoch 3  - Iter 310  - Training Time: 96366.61679506302 -     Loss: 1.4661226272583008      -    Accuracy: 0.5171925566343042\n",
      "Epoch 3  - Iter 320  - Training Time: 96730.36533498764 -     Loss: 1.4646577835083008      -    Accuracy: 0.5158699059561128\n",
      "Epoch 3  - Iter 330  - Training Time: 97101.04716205597 -     Loss: 1.4651708602905273      -    Accuracy: 0.5151025835866262\n",
      "Epoch 3  - Iter 340  - Training Time: 97473.54405808449 -     Loss: 1.464452862739563      -    Accuracy: 0.5142883480825958\n",
      "Epoch 3  - Iter 350  - Training Time: 97815.54924297333 -     Loss: 1.4627258777618408      -    Accuracy: 0.514237106017192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3  - Iter 360  - Training Time: 98156.7338719368 -     Loss: 1.462003231048584      -    Accuracy: 0.5164519498607242\n",
      "Epoch 3  - Iter 370  - Training Time: 98517.7891061306 -     Loss: 1.4615613222122192      -    Accuracy: 0.517699864498645\n",
      "Epoch 3  - Iter 380  - Training Time: 98868.4282488823 -     Loss: 1.4614312648773193      -    Accuracy: 0.5194591029023746\n",
      "Epoch 3  - Iter 390  - Training Time: 99206.91242098808 -     Loss: 1.4612977504730225      -    Accuracy: 0.5204048843187661\n",
      "Epoch 3  - Iter 400  - Training Time: 99544.8468902111 -     Loss: 1.4608362913131714      -    Accuracy: 0.5187186716791979\n",
      "Epoch 3  - Iter 410  - Training Time: 99881.82496094704 -     Loss: 1.4607396125793457      -    Accuracy: 0.519789119804401\n",
      "Epoch 3  - Iter 420  - Training Time: 100217.7894320488 -     Loss: 1.4596943855285645      -    Accuracy: 0.5214051312649165\n",
      "Epoch 3  - Iter 430  - Training Time: 100552.69215393066 -     Loss: 1.4588743448257446      -    Accuracy: 0.5219259906759907\n",
      "Epoch 3  - Iter 440  - Training Time: 100887.4648501873 -     Loss: 1.4584259986877441      -    Accuracy: 0.5222095671981777\n",
      "Epoch 3  - Iter 450  - Training Time: 101221.88904595375 -     Loss: 1.4587570428848267      -    Accuracy: 0.5206709354120267\n",
      "Epoch 3  - Iter 460  - Training Time: 101556.25329613686 -     Loss: 1.4573359489440918      -    Accuracy: 0.5226034858387799\n",
      "Epoch 3  - Iter 470  - Training Time: 101890.69409918785 -     Loss: 1.4573339223861694      -    Accuracy: 0.5245868869936035\n",
      "Epoch 3  - Iter 480  - Training Time: 102225.62951421738 -     Loss: 1.4579226970672607      -    Accuracy: 0.5242693110647182\n",
      "Epoch 3  - Iter 490  - Training Time: 102559.83579516411 -     Loss: 1.458137035369873      -    Accuracy: 0.522239263803681\n",
      "Epoch 3  - Iter 500  - Training Time: 102894.83678889275 -     Loss: 1.4576791524887085      -    Accuracy: 0.5218562124248497\n",
      "Epoch 3  - Iter 510  - Training Time: 103229.25686383247 -     Loss: 1.458196759223938      -    Accuracy: 0.5223477406679764\n",
      "Epoch 3  - Iter 520  - Training Time: 103564.24939608574 -     Loss: 1.4586864709854126      -    Accuracy: 0.5207731213872833\n",
      "Epoch 3  - Iter 530  - Training Time: 103899.26207304001 -     Loss: 1.4588500261306763      -    Accuracy: 0.5183128544423441\n",
      "Epoch 3  - Iter 540  - Training Time: 104234.11644887924 -     Loss: 1.4578886032104492      -    Accuracy: 0.5184369202226345\n",
      "Epoch 3  - Iter 550  - Training Time: 104568.80735993385 -     Loss: 1.4560452699661255      -    Accuracy: 0.5200364298724954\n",
      "Epoch 3  - Iter 560  - Training Time: 104906.34269499779 -     Loss: 1.4560991525650024      -    Accuracy: 0.5196779964221825\n",
      "Epoch 3  - Iter 570  - Training Time: 105271.8385848999 -     Loss: 1.4558061361312866      -    Accuracy: 0.5192223198594025\n",
      "Epoch 3  - Iter 580  - Training Time: 105648.65810918808 -     Loss: 1.4545971155166626      -    Accuracy: 0.520185664939551\n",
      "Epoch 3  - Iter 590  - Training Time: 106019.89787602425 -     Loss: 1.454369068145752      -    Accuracy: 0.5226018675721562\n",
      "Epoch 3  - Iter 600  - Training Time: 106391.94241094589 -     Loss: 1.4547266960144043      -    Accuracy: 0.5226940734557596\n",
      "Epoch 3  - Iter 610  - Training Time: 106760.99982905388 -     Loss: 1.4543908834457397      -    Accuracy: 0.5223214285714286\n",
      "Epoch 3  - Iter 620  - Training Time: 107131.82318401337 -     Loss: 1.453611135482788      -    Accuracy: 0.5221627625201939\n",
      "Epoch 3  - Iter 630  - Training Time: 107499.44772791862 -     Loss: 1.4532008171081543      -    Accuracy: 0.5233505564387917\n",
      "Epoch 3  - Iter 640  - Training Time: 107866.47498512268 -     Loss: 1.4526257514953613      -    Accuracy: 0.5239143192488263\n",
      "Epoch 3  - Iter 650  - Training Time: 108232.17974305153 -     Loss: 1.4528576135635376      -    Accuracy: 0.5239791987673343\n",
      "Epoch 3  - Iter 660  - Training Time: 108601.5635650158 -     Loss: 1.4525333642959595      -    Accuracy: 0.5238524279210925\n",
      "Epoch 3  - Iter 670  - Training Time: 108970.38544392586 -     Loss: 1.451743721961975      -    Accuracy: 0.523776158445441\n",
      "Epoch 3  - Iter 680  - Training Time: 109339.5275979042 -     Loss: 1.4512087106704712      -    Accuracy: 0.5234259941089838\n",
      "Epoch 3  - Iter 690  - Training Time: 109707.80410408974 -     Loss: 1.4509690999984741      -    Accuracy: 0.5245827285921626\n",
      "Epoch 3  - Iter 700  - Training Time: 110078.98054003716 -     Loss: 1.4512293338775635      -    Accuracy: 0.5240075107296137\n",
      "Epoch 3  - Iter 710  - Training Time: 110444.84174919128 -     Loss: 1.4502652883529663      -    Accuracy: 0.5231840620592384\n",
      "Epoch 3  - Iter 720  - Training Time: 110809.86842679977 -     Loss: 1.4505096673965454      -    Accuracy: 0.52355702364395\n",
      "Epoch 3  - Iter 730  - Training Time: 111176.93930602074 -     Loss: 1.4502463340759277      -    Accuracy: 0.5238340192043895\n",
      "Epoch 3  - Iter 740  - Training Time: 111537.313778162 -     Loss: 1.450242280960083      -    Accuracy: 0.5233423545331529\n",
      "Epoch 3  - Iter 750  - Training Time: 111898.04515123367 -     Loss: 1.4503321647644043      -    Accuracy: 0.5226552069425902\n",
      "Epoch 3  - Iter 760  - Training Time: 112243.27452516556 -     Loss: 1.4495388269424438      -    Accuracy: 0.5216979578392622\n",
      "Epoch 3  - Iter 770  - Training Time: 112597.2285759449 -     Loss: 1.4500828981399536      -    Accuracy: 0.522594278283485\n",
      "Epoch 3  - Iter 780  - Training Time: 112960.55029392242 -     Loss: 1.4499256610870361      -    Accuracy: 0.5232268934531451\n",
      "Iter 100     -     Loss: 1.433145353794098     -      Accuracy: 0.5612373737373737\n",
      "Iter 200     -     Loss: 1.4328691446781159     -      Accuracy: 0.5557474874371859\n",
      "Iter 300     -     Loss: 1.4272641317049661     -      Accuracy: 0.5547658862876255\n",
      "END  ---  Epoch 2  ---  Training Error: 1.449142575263977  ---   Validation Error: 1.4243862720401697\n"
     ]
    }
   ],
   "source": [
    "# train and check validation\n",
    "# mod.train(True)\n",
    "train_loss, train_acc, val_loss, val_acc = train(mod, train_loader, train_len, val_loader, val_len, epochs=3, learning_rate=0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.5332056866277515, 1.4746790392624447, 1.4243862720401697]\n",
      "[0.5885741718674988, 0.5621699471915507, 0.5566490638502161]\n"
     ]
    }
   ],
   "source": [
    "print(val_loss)\n",
    "print(val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'finetuned_BERT_lr_5e-5_bs_32_epochs_3.mod'\n",
    "torch.save(mod, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_test(model, loader, loader_len):\n",
    "  \"\"\"Evaluate the network model based on validation set.\n",
    "  \"\"\"\n",
    "  #model.train(False)\n",
    "  model.eval() # go into evaluation mode\n",
    "  acc, err = 0, 0\n",
    "  with torch.no_grad():\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    for iter, (tokens, attention_mask, rating) in enumerate(loader):\n",
    "      pred = model(tokens, attention_mask)\n",
    "      # loss = criterion(nn.LogSoftmax(pred, dim=1), rating)\n",
    "      pred_temp1 = pred.argmax(dim=1)\n",
    "      loss = mean_squared_error(pred_temp1.numpy(), rating.numpy())\n",
    "      total_loss += loss\n",
    "      total_acc += get_test_accuracy(pred_temp1, rating)\n",
    "      if (iter + 1) % 100 == 0:\n",
    "        print(\"Iter {}     -     Loss: {}     -       Accuracy: {}\".format(iter+1, total_loss / (iter+1), total_acc / (iter+1)))\n",
    "    \n",
    "    err = (total_loss) / (iter + 1)\n",
    "    acc = (total_acc) / (loader_len) # the total number of correctly predicted \n",
    "    return err, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_accuracy(pred, label):\n",
    "  # determine the index of the most likely rating\n",
    "  # index = torch.argmin(pred, dim = 1)\n",
    "  # return the number of correctly predicted ratings / number of total examples in a batch\n",
    "  return (pred==label).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 100     -     Loss: 2.5253125     -       Accuracy: 18.15\n",
      "Iter 200     -     Loss: 2.59296875     -       Accuracy: 17.8\n",
      "Iter 300     -     Loss: 2.6044791666666667     -       Accuracy: 17.866666666666667\n"
     ]
    }
   ],
   "source": [
    "# to test the trained model on the test set\n",
    "test_err, test_acc = evaluate_test(mod, test_loader, test_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.646703608979824\n",
      "0.5587293967034725\n"
     ]
    }
   ],
   "source": [
    "print(test_err)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the Yelp dataset\n",
    "df = pd.read_csv('yelp.csv')\n",
    "df = df[['text', 'stars']]\n",
    "df = df.rename(columns={'text':'reviewText', 'stars':'overall'})\n",
    "df_len = len(df)\n",
    "# limit to only 1000 examples\n",
    "yelp_inds = np.random.randint(0, df_len, size=1000) # pick\n",
    "df = df.iloc[yelp_inds] # now 1000 random examples\n",
    "# load df for Yelp into OurDataset\n",
    "yelp_data = OurDataset(df, 512)\n",
    "yelp_loader = DataLoader(yelp_data, batch_size=32)\n",
    "test_yelp_err, test_yelp_acc = evaluate_test(mod, yelp_loader, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6171875\n",
      "0.336\n"
     ]
    }
   ],
   "source": [
    "print(test_yelp_err)\n",
    "print(test_yelp_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import SST dataset\n",
    "sst_test = pd.read_csv('sst_test.txt', sep='\\t', header=None, names=['truth', 'text'])\n",
    "sst_test['truth'] = sst_test['truth'].str.replace('__label__', '')\n",
    "sst_test['truth'] = sst_test['truth'].astype(int).astype('category')\n",
    "sst_test = sst_test.rename(columns={'text':'reviewText', 'truth':'overall'})\n",
    "# limit to only 1000 examples\n",
    "sst_len = len(sst_test)\n",
    "sst_inds = np.random.randint(0, sst_len, size=1000)\n",
    "sst_test = sst_test.iloc[sst_inds] # now 1000 random examples\n",
    "# load df for SST into OurDataset\n",
    "sst_data = OurDataset(sst_test, 512)\n",
    "sst_loader = DataLoader(sst_data, batch_size=32)\n",
    "test_sst_err, test_sst_acc = evaluate_test(mod, sst_loader, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7314453125\n",
      "0.25\n"
     ]
    }
   ],
   "source": [
    "print(test_sst_err)\n",
    "print(test_sst_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
